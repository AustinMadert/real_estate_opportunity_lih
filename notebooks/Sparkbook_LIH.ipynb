{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Identifying Real Estate Opportunity for Low Income Housing Programs\n",
    "\n",
    "This is a rework of an analysis originally done using the traditional python scientific suite of modules. Here the bulk of the analysis is accomplished using Pyspark in order to take advantage of some of its features including the ML package and its pipelining capabilities.\n",
    "\n",
    "To reiterate the goal of this analysis, we are taking Austin, TX housing data and creating a scoring model that will allow us pick the top 100 properties that best fit low income housing program goals. We begin by importing the necessary packages, creating a spark context, and uploading our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import FloatType, ArrayType, DoubleType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from math import cos, sqrt\n",
    "import geopandas as gpd\n",
    "from helper_functions.plotly_plotting import plot_austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trulia_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"mode\", \"failFast\")\\\n",
    "    .load(\"data/prices.csv\")\n",
    "\n",
    "atx_addresses_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"mode\", \"failFast\")\\\n",
    "    .load(\"data/addresses_atx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>house_type</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>adj_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Condo</td>\n",
       "      <td>280000</td>\n",
       "      <td>1427</td>\n",
       "      <td>196.215837</td>\n",
       "      <td>4159 STECK AVENUE AUSTIN TX 78759</td>\n",
       "      <td>30.376725</td>\n",
       "      <td>-97.757702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Single-Family Home</td>\n",
       "      <td>299000</td>\n",
       "      <td>2224</td>\n",
       "      <td>134.442446</td>\n",
       "      <td>15810 DE PEER COVE AUSTIN TX 78717</td>\n",
       "      <td>30.492540</td>\n",
       "      <td>-97.740365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single-Family Home</td>\n",
       "      <td>329900</td>\n",
       "      <td>1005</td>\n",
       "      <td>328.258706</td>\n",
       "      <td>8201 LAZY LANE AUSTIN TX 78757</td>\n",
       "      <td>30.355568</td>\n",
       "      <td>-97.717455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single-Family Home</td>\n",
       "      <td>229874</td>\n",
       "      <td>1499</td>\n",
       "      <td>153.351568</td>\n",
       "      <td>14511 FITZGIBBON DRIVE AUSTIN TX 78725</td>\n",
       "      <td>30.234300</td>\n",
       "      <td>-97.588498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single-Family Home</td>\n",
       "      <td>237501</td>\n",
       "      <td>1295</td>\n",
       "      <td>183.398456</td>\n",
       "      <td>10304 BANKHEAD DRIVE AUSTIN TX 78757</td>\n",
       "      <td>30.351538</td>\n",
       "      <td>-97.732571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0  bathrooms  bedrooms          house_type   price  sqft  price_per_sqft  \\\n",
       "0    0        2.0       3.0               Condo  280000  1427      196.215837   \n",
       "1    1        3.0       4.0  Single-Family Home  299000  2224      134.442446   \n",
       "2    2        1.0       3.0  Single-Family Home  329900  1005      328.258706   \n",
       "3    3        2.0       3.0  Single-Family Home  229874  1499      153.351568   \n",
       "4    4        2.0       3.0  Single-Family Home  237501  1295      183.398456   \n",
       "\n",
       "                              adj_address   latitude  longitude  \n",
       "0       4159 STECK AVENUE AUSTIN TX 78759  30.376725 -97.757702  \n",
       "1      15810 DE PEER COVE AUSTIN TX 78717  30.492540 -97.740365  \n",
       "2          8201 LAZY LANE AUSTIN TX 78757  30.355568 -97.717455  \n",
       "3  14511 FITZGIBBON DRIVE AUSTIN TX 78725  30.234300 -97.588498  \n",
       "4    10304 BANKHEAD DRIVE AUSTIN TX 78757  30.351538 -97.732571  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here using pandas to display the result because Pyspark dataframes sometimes take up too much space!\n",
    "pandas_df = pd.DataFrame(trulia_df.take(5), columns=trulia_df.columns)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rows: ', 623, 'Columns: ', 10)\n"
     ]
    }
   ],
   "source": [
    "print((\"Rows: \", trulia_df.count(),\"Columns: \", len(trulia_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+--------------------+\n",
      "|_c0|               lat|               lon|         adj_address|\n",
      "+---+------------------+------------------+--------------------+\n",
      "|  1|30.186914399999996|       -97.9280313|12200 PRATOLINA D...|\n",
      "|  3|         30.352751|-97.95679799999999|15302 DOROTHY DRI...|\n",
      "|  4|30.169753200000002|-97.83318670000001|2515 DREW LANEAUS...|\n",
      "|  5|30.354369199999997|       -97.9577997|15404 JOSEPH DRIV...|\n",
      "|  7|30.399155600000004|       -97.8516461|11203 RANCH ROAD ...|\n",
      "+---+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atx_addresses_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rows: ', 140386, 'Columns: ', 4)\n"
     ]
    }
   ],
   "source": [
    "print((\"Rows: \", atx_addresses_df.count(), \"Columns: \", len(atx_addresses_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "atx_addresses_df = atx_addresses_df.withColumnRenamed('lat', 'latitude').withColumnRenamed('lon', 'longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now that we have our data in Pyspark dataframes, we will begin to build our data pipeline. The pipeline functionality allows us to plan out several data transformations and have Spark optimize the workload over all the transformations for us.\n",
    "\n",
    "In Pyspark, almost all of the ML package's algorithms require the input data to be formatted in a dataframe column consisting of vectors of the input features. Convention is to name this column \"features\". And so we'll use the VectorAssembler to accomplish this. For our purposes, we need to use training data that we scraped from Trulia.com to train a regressor and then use the model to regress housing data onto a larger dataset of properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stages list to create a plan for the pipeline\n",
    "stages = []\n",
    "assemblerInputs = ['latitude', 'longitude']\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_040bbcbd7a24]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- price_per_sqft: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate pipeline and fit it to our data set\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(trulia_df)\n",
    "train_df = pipelineModel.transform(trulia_df)\n",
    "train_df = train_df.select('features', 'price_per_sqft')\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- adj_address: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we repeat the above step, but for our dataframe of expanded property data so that we can use or regressor on it\n",
    "pipelineModel = pipeline.fit(atx_addresses_df)\n",
    "test_df = pipelineModel.transform(atx_addresses_df)\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>adj_address</th>\n",
       "      <th>features</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30.186914</td>\n",
       "      <td>-97.928031</td>\n",
       "      <td>12200 PRATOLINA DRIVEAUSTIN TX 78739</td>\n",
       "      <td>[30.186914399999996, -97.9280313]</td>\n",
       "      <td>196.793132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>30.352751</td>\n",
       "      <td>-97.956798</td>\n",
       "      <td>15302 DOROTHY DRIVEAUSTIN TX 78734</td>\n",
       "      <td>[30.352751, -97.95679799999999]</td>\n",
       "      <td>201.806851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>30.169753</td>\n",
       "      <td>-97.833187</td>\n",
       "      <td>2515 DREW LANEAUSTIN TX 78748</td>\n",
       "      <td>[30.169753200000002, -97.83318670000001]</td>\n",
       "      <td>181.144136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>30.354369</td>\n",
       "      <td>-97.957800</td>\n",
       "      <td>15404 JOSEPH DRIVEAUSTIN TX 78734</td>\n",
       "      <td>[30.354369199999997, -97.9577997]</td>\n",
       "      <td>201.806851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>30.399156</td>\n",
       "      <td>-97.851646</td>\n",
       "      <td>11203 RANCH ROAD 2222AUSTIN TX 78730</td>\n",
       "      <td>[30.399155600000004, -97.8516461]</td>\n",
       "      <td>239.403664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0   latitude  longitude                           adj_address  \\\n",
       "0    1  30.186914 -97.928031  12200 PRATOLINA DRIVEAUSTIN TX 78739   \n",
       "1    3  30.352751 -97.956798    15302 DOROTHY DRIVEAUSTIN TX 78734   \n",
       "2    4  30.169753 -97.833187         2515 DREW LANEAUSTIN TX 78748   \n",
       "3    5  30.354369 -97.957800     15404 JOSEPH DRIVEAUSTIN TX 78734   \n",
       "4    7  30.399156 -97.851646  11203 RANCH ROAD 2222AUSTIN TX 78730   \n",
       "\n",
       "                                   features  prediction  \n",
       "0         [30.186914399999996, -97.9280313]  196.793132  \n",
       "1           [30.352751, -97.95679799999999]  201.806851  \n",
       "2  [30.169753200000002, -97.83318670000001]  181.144136  \n",
       "3         [30.354369199999997, -97.9577997]  201.806851  \n",
       "4         [30.399155600000004, -97.8516461]  239.403664  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pyspark will return a dataframe to us with the prediction column of generated outputs based on the features \n",
    "# column that we created\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='price_per_sqft', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "pandas_df = pd.DataFrame(gbt_predictions.take(5), columns=gbt_predictions.columns)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we need to complete the above process to train multiple regressors and regress multiple features on to our\n",
    "# expanded dataset, we can create a function that allows us to loop over the above, passing the result df back\n",
    "def regress_new_feature (train_df, test_df, features, label, maxIter=10):\n",
    "    stages = []\n",
    "    assemblerInputs = features\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "    stages += [assembler]\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(train_df)\n",
    "    train = pipelineModel.transform(train_df)\n",
    "    pipelineModel = pipeline.fit(test_df)\n",
    "    test = pipelineModel.transform(test_df)\n",
    "    \n",
    "    gbt = GBTRegressor(featuresCol='features', labelCol=label, maxIter=maxIter)\n",
    "    gbt_model = gbt.fit(train)\n",
    "    prediction_df = gbt_model.transform(test)\n",
    "    \n",
    "    prediction_df = prediction_df.withColumnRenamed('prediction', label).drop('features')\n",
    "    \n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns we would like to regress onto the expanded dataset\n",
    "train_cols = ['price_per_sqft', 'price', 'bedrooms', 'bathrooms']\n",
    "\n",
    "# The features we will start with\n",
    "features = ['latitude', 'longitude']\n",
    "\n",
    "# Our original dataframes\n",
    "train_df = trulia_df\n",
    "test_df = atx_addresses_df\n",
    "\n",
    "# A loop that uses the above function and passes the result back through the function for each newly regressed column\n",
    "for col in train_cols:\n",
    "    test_df = regress_new_feature(train_df, test_df, features, col, 50)\n",
    "    features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>adj_address</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30.186914</td>\n",
       "      <td>-97.928031</td>\n",
       "      <td>12200 PRATOLINA DRIVEAUSTIN TX 78739</td>\n",
       "      <td>197.788176</td>\n",
       "      <td>6.042956e+05</td>\n",
       "      <td>4.086110</td>\n",
       "      <td>3.408979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>30.352751</td>\n",
       "      <td>-97.956798</td>\n",
       "      <td>15302 DOROTHY DRIVEAUSTIN TX 78734</td>\n",
       "      <td>188.078154</td>\n",
       "      <td>5.729904e+05</td>\n",
       "      <td>3.779476</td>\n",
       "      <td>3.489850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>30.169753</td>\n",
       "      <td>-97.833187</td>\n",
       "      <td>2515 DREW LANEAUSTIN TX 78748</td>\n",
       "      <td>185.093968</td>\n",
       "      <td>3.714479e+05</td>\n",
       "      <td>3.213077</td>\n",
       "      <td>2.215508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>30.354369</td>\n",
       "      <td>-97.957800</td>\n",
       "      <td>15404 JOSEPH DRIVEAUSTIN TX 78734</td>\n",
       "      <td>188.078154</td>\n",
       "      <td>5.729904e+05</td>\n",
       "      <td>3.779476</td>\n",
       "      <td>3.489850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>30.399156</td>\n",
       "      <td>-97.851646</td>\n",
       "      <td>11203 RANCH ROAD 2222AUSTIN TX 78730</td>\n",
       "      <td>292.769085</td>\n",
       "      <td>1.617012e+06</td>\n",
       "      <td>4.617920</td>\n",
       "      <td>5.274092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0   latitude  longitude                           adj_address  \\\n",
       "0    1  30.186914 -97.928031  12200 PRATOLINA DRIVEAUSTIN TX 78739   \n",
       "1    3  30.352751 -97.956798    15302 DOROTHY DRIVEAUSTIN TX 78734   \n",
       "2    4  30.169753 -97.833187         2515 DREW LANEAUSTIN TX 78748   \n",
       "3    5  30.354369 -97.957800     15404 JOSEPH DRIVEAUSTIN TX 78734   \n",
       "4    7  30.399156 -97.851646  11203 RANCH ROAD 2222AUSTIN TX 78730   \n",
       "\n",
       "   price_per_sqft         price  bedrooms  bathrooms  \n",
       "0      197.788176  6.042956e+05  4.086110   3.408979  \n",
       "1      188.078154  5.729904e+05  3.779476   3.489850  \n",
       "2      185.093968  3.714479e+05  3.213077   2.215508  \n",
       "3      188.078154  5.729904e+05  3.779476   3.489850  \n",
       "4      292.769085  1.617012e+06  4.617920   5.274092  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame(test_df.take(5), columns=test_df.columns)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rows: ', 140386, 'Columns: ', 8)\n"
     ]
    }
   ],
   "source": [
    "print((\"Rows: \", test_df.count(),\"Columns: \", len(test_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a full dataset to work with and can begin feature engineering in earnest. As mentioned in the project walkthrough, we'll need to find the minimum distance to public transportation for each address in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use geopandas to read shapefile of all bus stops in ATX, then put them into a list\n",
    "atx_bus_map = gpd.read_file('data/Shapefiles_20-_20JANUARY_202018/Stops/Stops.shp')\n",
    "bus_stops = atx_bus_map[['LATITUDE', 'LONGITUDE']]\n",
    "bus_stops = [(lat, lon) for lat, lon in zip(atx_bus_map['LATITUDE'], atx_bus_map['LONGITUDE'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for our distance calculation between two points\n",
    "def two_point_haversine(origin, destination):\n",
    "    # calculates the distance between two points (lat, lngs) on a great circle, or on the \n",
    "    # surface of a sphere (in this case the sphere is planet earth)\n",
    "    # units in km\n",
    "    lat1, lng1 = origin\n",
    "    lat2, lng2 = destination\n",
    "    deglen = 110.25\n",
    "    x = lat1 - lat2\n",
    "    y = (lng1 - (lng2))*cos(lng2)\n",
    "    return deglen*sqrt(x*x + y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to run through all the bus_stops for each property\n",
    "def min_dist_to_transportation(homes, bus_stops):\n",
    "    return [min([two_point_haversine(stop, home) for stop in bus_stops]) for home in homes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the distance calculation in python list\n",
    "pandf = test_df.toPandas()\n",
    "homes = list(zip(pandf.latitude, pandf.longitude))\n",
    "min_dist = min_dist_to_transportation(homes, bus_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index column in our Pyspark dataframe using SQL\n",
    "test_df.createOrReplaceTempView('testTable')\n",
    "\n",
    "query = '''\n",
    "SELECT *, ROW_NUMBER() OVER(ORDER BY adj_address) AS index\n",
    "FROM testTable\n",
    "'''\n",
    "\n",
    "test_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a simple User Defined Function (UDF) to create a column including our min_dist calculations into the dataframe\n",
    "def add_min_dist(idx):\n",
    "    return min_dist[idx - 1]\n",
    "\n",
    "min_dist_udf = F.udf(add_min_dist, FloatType())\n",
    "\n",
    "\n",
    "test_df = test_df.withColumn('min_dist', min_dist_udf('index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new pipeline for our KMeans cluster\n",
    "stages = []\n",
    "assemblerInputs = ['latitude', 'longitude']\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "stages += [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(test_df)\n",
    "train_df = pipelineModel.transform(test_df)\n",
    "train_df = train_df.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[30.1869143999999...|        14|\n",
      "|[30.352751,-97.95...|         1|\n",
      "|[30.1697532000000...|         2|\n",
      "|[30.3543691999999...|         1|\n",
      "|[30.3991556000000...|        10|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From the walkthrough, we cluster latitude and longitude with K=15\n",
    "kmeans = KMeans(maxIter=300).setK(15).setSeed(1)\n",
    "\n",
    "kmeans_model = kmeans.fit(train_df)\n",
    "\n",
    "predictions = kmeans_model.transform(train_df)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|cluster|            centroid|\n",
      "+-------+--------------------+\n",
      "|      0|[30.1672362337052...|\n",
      "|      1|[30.3345713518887...|\n",
      "|      2|[30.1638933586402...|\n",
      "|      3|[30.3442655765284...|\n",
      "|      4|[30.3992073787710...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We create a dataframe with each centroid and its number (this is important because each centroid is a \n",
    "# latitude/longitude pair, and we'll need them for plotting later)\n",
    "numbered_clusters = []\n",
    "for num, cluster in enumerate(kmeans_model.clusterCenters()):\n",
    "    numbered_clusters.append([num, [float(cluster[0]), float(cluster[1])]])\n",
    "centroids = spark.createDataFrame(numbered_clusters, ['cluster', 'centroid'])\n",
    "centroids.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can join the centroids back onto the training dataframe using the prediction from the KMeans algorithm \n",
    "# (predictions from KMeans are cluster IDs)\n",
    "predictions.createOrReplaceTempView('preds')\n",
    "centroids.createOrReplaceTempView('centroids')\n",
    "\n",
    "# Creating an index to allow us to join to the original dataframe below\n",
    "query = '''\n",
    "SELECT *, ROW_NUMBER() OVER(ORDER BY features) AS index\n",
    "FROM preds AS p\n",
    "JOIN centroids AS c ON p.prediction=c.cluster\n",
    "'''\n",
    "\n",
    "preds = spark.sql(query).drop('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+\n",
      "|            features|prediction|            centroid|index|\n",
      "+--------------------+----------+--------------------+-----+\n",
      "|[28.3318666,-98.1...|         2|[30.1638933586402...|    1|\n",
      "|[29.5116391,-94.4...|        13|[30.2487794564482...|    2|\n",
      "|[29.5555698999999...|        13|[30.2487794564482...|    3|\n",
      "|[29.5583186999999...|        13|[30.2487794564482...|    4|\n",
      "|[29.6142135,-95.5...|        13|[30.2487794564482...|    5|\n",
      "+--------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the predictions, complete with centroids, back to the original dataframe\n",
    "preds.createOrReplaceTempView('preds')\n",
    "test_df.createOrReplaceTempView('original')\n",
    "\n",
    "query = '''\n",
    "SELECT *, p.prediction AS cluster FROM original AS o\n",
    "INNER JOIN preds AS p ON o.index=p.index\n",
    "'''\n",
    "\n",
    "df = spark.sql(query).drop('index').select('latitude', 'longitude', 'price_per_sqft', 'min_dist', 'centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+---------+--------------------+\n",
      "|          latitude|         longitude|    price_per_sqft| min_dist|            centroid|\n",
      "+------------------+------------------+------------------+---------+--------------------+\n",
      "|30.312099800000002|-97.72457390000001| 399.9647760710277|3.1569898|[30.1638933586402...|\n",
      "|30.376518199999996|        -97.937327|260.05716049083173|11.432891|[30.2487794564482...|\n",
      "|         30.257994|-98.04043399999999| 212.4208338172844|0.7196791|[30.2487794564482...|\n",
      "|        30.3413012|       -97.6807191|  131.892550554785|11.240022|[30.2487794564482...|\n",
      "|30.351137899999994|       -97.8331579|314.00411531624593|0.7368289|[30.2487794564482...|\n",
      "+------------------+------------------+------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the walkthrough, calculate distance to centroid for each property, reusing our distance function from above\n",
    "def distance_to_centroid(arr, centroid):\n",
    "    return two_point_haversine(arr, centroid)\n",
    "\n",
    "centroid_udf = F.udf(distance_to_centroid, FloatType())\n",
    "\n",
    "df = df.withColumn('dist_to_cent', centroid_udf(F.array(\"latitude\", \"longitude\"), \"centroid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>centroid</th>\n",
       "      <th>dist_to_cent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.312100</td>\n",
       "      <td>-97.724574</td>\n",
       "      <td>399.964776</td>\n",
       "      <td>3.156990</td>\n",
       "      <td>[30.163893358640284, -97.83465163455125]</td>\n",
       "      <td>19.671022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.376518</td>\n",
       "      <td>-97.937327</td>\n",
       "      <td>260.057160</td>\n",
       "      <td>11.432891</td>\n",
       "      <td>[30.248779456448283, -97.58516499404504]</td>\n",
       "      <td>40.604568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.257994</td>\n",
       "      <td>-98.040434</td>\n",
       "      <td>212.420834</td>\n",
       "      <td>0.719679</td>\n",
       "      <td>[30.248779456448283, -97.58516499404504]</td>\n",
       "      <td>49.244881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.341301</td>\n",
       "      <td>-97.680719</td>\n",
       "      <td>131.892551</td>\n",
       "      <td>11.240022</td>\n",
       "      <td>[30.248779456448283, -97.58516499404504]</td>\n",
       "      <td>14.520093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.351138</td>\n",
       "      <td>-97.833158</td>\n",
       "      <td>314.004115</td>\n",
       "      <td>0.736829</td>\n",
       "      <td>[30.248779456448283, -97.58516499404504]</td>\n",
       "      <td>29.096413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude  price_per_sqft   min_dist  \\\n",
       "0  30.312100 -97.724574      399.964776   3.156990   \n",
       "1  30.376518 -97.937327      260.057160  11.432891   \n",
       "2  30.257994 -98.040434      212.420834   0.719679   \n",
       "3  30.341301 -97.680719      131.892551  11.240022   \n",
       "4  30.351138 -97.833158      314.004115   0.736829   \n",
       "\n",
       "                                   centroid  dist_to_cent  \n",
       "0  [30.163893358640284, -97.83465163455125]     19.671022  \n",
       "1  [30.248779456448283, -97.58516499404504]     40.604568  \n",
       "2  [30.248779456448283, -97.58516499404504]     49.244881  \n",
       "3  [30.248779456448283, -97.58516499404504]     14.520093  \n",
       "4  [30.248779456448283, -97.58516499404504]     29.096413  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame(df.take(5), columns=df.columns)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Calculation\n",
    "\n",
    "Now that we've engineered features for the minimum distance to transportation and the distance to centroid, we can begin developing our score calculation. We will normalize price_per_sqft, min_dist, and dist_to_cent, and then take the sum of all of these as our ranking score. This will set us up to begin plotting our selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UDF to sum our normalized columns. The input here is a vector, because will be using another pipeline\n",
    "# which, as we've seen above outputs a series of vectors based on the input columns.\n",
    "def sum_(vector):\n",
    "    try:\n",
    "        result = sum(list([float(val) for val in vector]))\n",
    "        if result is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return result\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "scorer = F.udf(sum_, DoubleType())\n",
    "# Must register with spark to make available for sql queries - see below\n",
    "spark.udf.register('scorer', scorer)\n",
    "\n",
    "# Here we create a new pipeline, then use a sql query to apply our scorer UDF\n",
    "def score_calculation(train_df, features):\n",
    "    stages = []\n",
    "    assemblerInputs = features\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "    # Pyspark has builtin normalization functionality for feature columns. The MinMaxScaler is nifty!\n",
    "    # Notice our inputCol will be the outputCol from the assembler\n",
    "    scaler = MinMaxScaler(inputCol=assembler.getOutputCol(), outputCol='scaled_features')\n",
    "    stages += [assembler, scaler]\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(train_df)\n",
    "    df = pipelineModel.transform(train_df)\n",
    "    \n",
    "    df.createOrReplaceTempView('table')\n",
    "    # We use a sql query to apply the scorer function as well as sort and take the top 100 best properties!\n",
    "    query = '''\n",
    "    SELECT latitude, longitude, scaled_features, scorer(scaled_features) AS score\n",
    "    FROM table\n",
    "    ORDER BY score DESC LIMIT 100\n",
    "    '''\n",
    "    result = spark.sql(query)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call our function with the input features and take a look at the output\n",
    "newdf = score_calculation(df, ['price_per_sqft', 'min_dist', 'dist_to_cent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+------------------+\n",
      "|          latitude|         longitude|     scaled_features|             score|\n",
      "+------------------+------------------+--------------------+------------------+\n",
      "|         30.321707|       -97.7371546|[0.14325306946584...| 1.190121379927792|\n",
      "|30.336747999999996|        -97.782667|[1.0,0.0286431011...| 1.083053282206181|\n",
      "|30.337832199999998|-97.77931550000001|[1.0,0.0345162546...|1.0683912100235349|\n",
      "|         30.337458|       -97.7766735|[1.0,0.0104056139...|1.0664375514756954|\n",
      "|        30.3392972|       -97.7800517|[1.0,0.0101582440...|1.0652915402189165|\n",
      "+------------------+------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "We'll take the coordinates from our top 100 properties and plot them using a plotly wrapper function (found in helper_functions in this repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Pyspark columns and create typed lists\n",
    "latitudes = newdf.select('latitude')\n",
    "latitudes = [float(row['latitude']) for row in latitudes.collect()]\n",
    "longitudes = newdf.select('longitude')\n",
    "longitudes = [float(row['longitude']) for row in longitudes.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use helper function\n",
    "plot_austin(latitudes, longitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Unfortunately, this analysis was done in a Pyspark enabled notebook in a docker container. Thus Plotly was acting finnicky and not allowing the Plotly map data to come through to my container. So, unsatisfying as it may be to slog through the above analysis to end up with no visuals, fear not! There are visuals in the project walkthrough that this notebook is based on. Oh and don't forget to checkout the plotly helper function!</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Final Thoughts\n",
    "\n",
    "As we've seen above, Pyspark is incredibly powerful for machine learning analyses. We drastically improved our ability to feature engineer a dataset using the pipeline capabilities from the ML package. A lot was also gained from Pyspark's integrated functionality around sql queries and User Defined Functions. \n",
    "\n",
    "Sadly, we weren't able to plot within this notebook, but please do refer to the project walkthrough as the Plotly module creates very nice maps, and the plots are included in that markdown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
