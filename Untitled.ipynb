{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Real Estate Opportunity for Low Income Housing Programs\n",
    "\n",
    "This is a rework of an analysis originally done using the traditional python scientific suite of modules. Here the bulk of the analysis is accomplished using Pyspark in order to take advantage of some of its features including the ML package and its pipelining capabilities.\n",
    "\n",
    "To reiterate the goal of this analysis, we are taking Austin, TX housing data and creating a scoring model that will allow us pick the top 100 properties that best fit low income housing program goals. We begin by importing the necessary packages, creating a spark context, and uploading our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import FloatType, ArrayType, DoubleType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from math import cos, sqrt\n",
    "import geopandas as gpd\n",
    "from helper_functions.plotly_plotting import plot_austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trulia_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"mode\", \"failFast\")\\\n",
    "    .load(\"data/prices.csv\")\n",
    "\n",
    "atx_addresses_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"mode\", \"failFast\")\\\n",
    "    .load(\"data/addresses_atx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------------------+------+----+------------------+--------------------+----------+-----------+\n",
      "|_c0|bathrooms|bedrooms|        house_type| price|sqft|    price_per_sqft|         adj_address|  latitude|  longitude|\n",
      "+---+---------+--------+------------------+------+----+------------------+--------------------+----------+-----------+\n",
      "|  0|      2.0|     3.0|             Condo|280000|1427| 196.2158374211633|4159 STECK AVENUE...|30.3767245|-97.7577025|\n",
      "|  1|      3.0|     4.0|Single-Family Home|299000|2224|134.44244604316546|15810 DE PEER COV...|  30.49254| -97.740365|\n",
      "|  2|      1.0|     3.0|Single-Family Home|329900|1005| 328.2587064676617|8201 LAZY LANE AU...| 30.355568| -97.717455|\n",
      "|  3|      2.0|     3.0|Single-Family Home|229874|1499|153.35156771180786|14511 FITZGIBBON ...|   30.2343| -97.588498|\n",
      "|  4|      2.0|     3.0|Single-Family Home|237501|1295| 183.3984555984556|10304 BANKHEAD DR...| 30.351538| -97.732571|\n",
      "+---+---------+--------+------------------+------+----+------------------+--------------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trulia_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rows: ', 623, 'Columns: ', 10)\n"
     ]
    }
   ],
   "source": [
    "print((\"Rows: \", trulia_df.count(),\"Columns: \", len(trulia_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+--------------------+\n",
      "|_c0|               lat|               lon|         adj_address|\n",
      "+---+------------------+------------------+--------------------+\n",
      "|  1|30.186914399999996|       -97.9280313|12200 PRATOLINA D...|\n",
      "|  3|         30.352751|-97.95679799999999|15302 DOROTHY DRI...|\n",
      "|  4|30.169753200000002|-97.83318670000001|2515 DREW LANEAUS...|\n",
      "|  5|30.354369199999997|       -97.9577997|15404 JOSEPH DRIV...|\n",
      "|  7|30.399155600000004|       -97.8516461|11203 RANCH ROAD ...|\n",
      "+---+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atx_addresses_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rows: ', 140386, 'Columns: ', 4)\n"
     ]
    }
   ],
   "source": [
    "print((\"Rows: \", atx_addresses_df.count(), \"Columns: \", len(atx_addresses_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "atx_addresses_df = atx_addresses_df.withColumnRenamed('lat', 'latitude').withColumnRenamed('lon', 'longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now that we have our data in Pyspark dataframes, we will begin to build our data pipeline. The pipeline functionality allows us to plan out several data transformations and have Spark optimize the workload over all the transformations for us.\n",
    "\n",
    "In Pyspark, almost all of the ML package's algorithms require the input data to be formatted in a dataframe column consisting of vectors of the input features. Convention is to name this column \"features\". And so we'll use the VectorAssembler to accomplish this. For our purposes, we need to use training data that we scraped from Trulia.com to train a regressor and then use the model to regress housing data onto a larger dataset of properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stages list to create a plan for the pipeline\n",
    "stages = []\n",
    "assemblerInputs = ['latitude', 'longitude']\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_17d6e9b5af69]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
